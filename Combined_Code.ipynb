{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Linear Regression**\n"
      ],
      "metadata": {
        "id": "4ZCqrzVZPaI9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 426
        },
        "id": "I16VaJR1PYYZ",
        "outputId": "ed53e82e-ac59-4fcc-90e5-faf1dbe8f028"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Input X contains NaN.\nLinearRegression does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-e21599d84db0>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;31m# Train the linear regression model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLinearRegression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;31m# Make predictions and evaluate the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1387\u001b[0m                 )\n\u001b[1;32m   1388\u001b[0m             ):\n\u001b[0;32m-> 1389\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfit_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1390\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1391\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_base.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    599\u001b[0m         \u001b[0maccept_sparse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpositive\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"csr\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"csc\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"coo\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    600\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 601\u001b[0;31m         X, y = validate_data(\n\u001b[0m\u001b[1;32m    602\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    603\u001b[0m             \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mvalidate_data\u001b[0;34m(_estimator, X, y, reset, validate_separately, skip_check_array, **check_params)\u001b[0m\n\u001b[1;32m   2959\u001b[0m             \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"y\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcheck_y_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2960\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2961\u001b[0;31m             \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_X_y\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcheck_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2962\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2963\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_X_y\u001b[0;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[1;32m   1368\u001b[0m     \u001b[0mensure_all_finite\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_deprecate_force_all_finite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mforce_all_finite\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mensure_all_finite\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1369\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1370\u001b[0;31m     X = check_array(\n\u001b[0m\u001b[1;32m   1371\u001b[0m         \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1372\u001b[0m         \u001b[0maccept_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maccept_sparse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_all_finite, ensure_non_negative, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[1;32m   1105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1106\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mensure_all_finite\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1107\u001b[0;31m             _assert_all_finite(\n\u001b[0m\u001b[1;32m   1108\u001b[0m                 \u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1109\u001b[0m                 \u001b[0minput_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36m_assert_all_finite\u001b[0;34m(X, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[1;32m    118\u001b[0m         \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 120\u001b[0;31m     _assert_all_finite_element_wise(\n\u001b[0m\u001b[1;32m    121\u001b[0m         \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m         \u001b[0mxp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mxp\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36m_assert_all_finite_element_wise\u001b[0;34m(X, xp, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[1;32m    167\u001b[0m                 \u001b[0;34m\"#estimators-that-handle-nan-values\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m             )\n\u001b[0;32m--> 169\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg_err\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    170\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Input X contains NaN.\nLinearRegression does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import random\n",
        "random.seed(42)\n",
        "\n",
        "data = pd.read_csv(\"Clean_Dataset.csv\")\n",
        "\n",
        "# Drop unnecessary columns and select features\n",
        "data = data.drop(columns=[\"Unnamed: 0\", \"flight\"])\n",
        "\n",
        "# One-hot encode categorical variables\n",
        "categorical_cols = [\"airline\", \"source_city\", \"departure_time\", \"stops\", \"arrival_time\", \"destination_city\", \"class\"]\n",
        "encoder = OneHotEncoder(drop='first', sparse_output=False)\n",
        "encoded_categorical = encoder.fit_transform(data[categorical_cols])\n",
        "\n",
        "# Combine encoded categorical variables with numerical ones\n",
        "numerical_cols = [\"duration\", \"days_left\"]\n",
        "X = np.hstack([encoded_categorical, data[numerical_cols].values])\n",
        "y = data[\"price\"]\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train the linear regression model\n",
        "model = LinearRegression()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions and evaluate the model\n",
        "y_pred = model.predict(X_test)\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "mse, r2\n",
        "\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.linear_model import Ridge, Lasso\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.compose import ColumnTransformer\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Load the dataset\n",
        "data = pd.read_csv(\"Clean_Dataset.csv\")\n",
        "\n",
        "# Drop unnecessary columns and select features\n",
        "data = data.drop(columns=[\"Unnamed: 0\", \"flight\"])\n",
        "\n",
        "# Separate features and target\n",
        "categorical_cols = [\"airline\", \"source_city\", \"departure_time\", \"stops\", \"arrival_time\", \"destination_city\", \"class\"]\n",
        "numerical_cols = [\"duration\", \"days_left\"]\n",
        "target_col = \"price\"\n",
        "\n",
        "# Log transform the target variable to reduce skewness\n",
        "data[target_col] = np.log1p(data[target_col])\n",
        "\n",
        "# Define preprocessing pipeline for numerical and categorical features\n",
        "numerical_transformer = Pipeline(steps=[\n",
        "    (\"scaler\", StandardScaler())\n",
        "])\n",
        "\n",
        "categorical_transformer = Pipeline(steps=[\n",
        "    (\"onehot\", OneHotEncoder(drop=\"first\", sparse_output=False))\n",
        "])\n",
        "\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        (\"num\", numerical_transformer, numerical_cols),\n",
        "        (\"cat\", categorical_transformer, categorical_cols)\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Define function to evaluate models\n",
        "def evaluate_model(model, X_train, X_test, y_train, y_test):\n",
        "    model.fit(X_train, y_train)\n",
        "    y_pred = model.predict(X_test)\n",
        "    mse = mean_squared_error(np.expm1(y_test), np.expm1(y_pred))  # Reverse log transformation\n",
        "    r2 = r2_score(np.expm1(y_test), np.expm1(y_pred))\n",
        "    return mse, r2\n",
        "\n",
        "# Split the data into train and test sets\n",
        "X = data[categorical_cols + numerical_cols]\n",
        "y = data[target_col]\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create pipelines for Ridge, Lasso, and Random Forest\n",
        "ridge_pipeline = Pipeline(steps=[\n",
        "    (\"preprocessor\", preprocessor),\n",
        "    (\"model\", Ridge())\n",
        "])\n",
        "\n",
        "lasso_pipeline = Pipeline(steps=[\n",
        "    (\"preprocessor\", preprocessor),\n",
        "    (\"model\", Lasso())\n",
        "])\n",
        "\n",
        "rf_pipeline = Pipeline(steps=[\n",
        "    (\"preprocessor\", preprocessor),\n",
        "    (\"model\", RandomForestRegressor(random_state=42, n_estimators=100))\n",
        "])\n",
        "\n",
        "# Hyperparameter tuning for Ridge and Lasso\n",
        "ridge_param_grid = {\"model__alpha\": [0.1, 1, 10, 100, 200]}\n",
        "lasso_param_grid = {\"model__alpha\": [0.01, 0.1, 1, 10, 100]}\n",
        "\n",
        "ridge_cv = GridSearchCV(ridge_pipeline, ridge_param_grid, scoring=\"neg_mean_squared_error\", cv=5)\n",
        "lasso_cv = GridSearchCV(lasso_pipeline, lasso_param_grid, scoring=\"neg_mean_squared_error\", cv=5)\n",
        "\n",
        "# Evaluate models\n",
        "ridge_mse, ridge_r2 = evaluate_model(ridge_cv, X_train, X_test, y_train, y_test)\n",
        "lasso_mse, lasso_r2 = evaluate_model(lasso_cv, X_train, X_test, y_train, y_test)\n",
        "rf_mse, rf_r2 = evaluate_model(rf_pipeline, X_train, X_test, y_train, y_test)\n",
        "\n",
        "# Print results\n",
        "print(\"Best Ridge Alpha:\", ridge_cv.best_params_[\"model__alpha\"])\n",
        "print(\"Ridge MSE:\", ridge_mse)\n",
        "print(\"Ridge R-squared:\", ridge_r2)\n",
        "\n",
        "print(\"Best Lasso Alpha:\", lasso_cv.best_params_[\"model__alpha\"])\n",
        "print(\"Lasso MSE:\", lasso_mse)\n",
        "print(\"Lasso R-squared:\", lasso_r2)\n",
        "\n",
        "print(\"Random Forest MSE:\", rf_mse)\n",
        "print(\"Random Forest R-squared:\", rf_r2)\n",
        "\n",
        "# Select 10 random samples from the test set\n",
        "random_indices = random.sample(range(len(X_test)), 10)\n",
        "X_sample = X_test.iloc[random_indices]\n",
        "y_sample_actual = np.expm1(y_test.iloc[random_indices])  # Reverse log transform\n",
        "\n",
        "# Fit the Random Forest pipeline and predict prices\n",
        "rf_pipeline.fit(X_train, y_train)\n",
        "y_sample_predicted = np.expm1(rf_pipeline.predict(X_sample))  # Reverse log transform\n",
        "\n",
        "# Create a DataFrame to display actual vs predicted prices\n",
        "predicted_vs_actual = pd.DataFrame({\n",
        "    \"Actual Price\": y_sample_actual.values,\n",
        "    \"Predicted Price\": y_sample_predicted\n",
        "})\n",
        "\n",
        "predicted_vs_actual"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Random Forest**"
      ],
      "metadata": {
        "id": "0Mu-5RlmQoDM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd  # For data manipulation\n",
        "import numpy as np  # For numerical operations\n",
        "from sklearn.model_selection import train_test_split  # For splitting the dataset\n",
        "from sklearn.ensemble import RandomForestRegressor  # For Random Forest model\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error  # For model evaluation\n",
        "from sklearn.preprocessing import LabelEncoder  # For encoding categorical variables\n",
        "\n",
        "# Load the dataset\n",
        "file_path = \"Clean_Dataset.csv\"\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "# Checking the first few rows of the dataset to understand its structure\n",
        "df.head()\n",
        "\n",
        "# Print column names to check\n",
        "print(df.columns)\n",
        "\n",
        "# Calculate the average price for each airline\n",
        "avg_price_by_airline = df.groupby('airline')['price'].mean()\n",
        "\n",
        "# Sort by price in descending order to find the most expensive airline\n",
        "most_expensive_airline = avg_price_by_airline.sort_values(ascending=False).head(1)\n",
        "\n",
        "# Print the result\n",
        "print(\"Most Expensive Airline:\")\n",
        "print(most_expensive_airline)\n",
        "\n",
        "# Subset the dataset to include only Vistara flights\n",
        "vistara_df = df[df['airline'] == 'Vistara']\n",
        "\n",
        "# Check the first few rows of the subsetted data\n",
        "vistara_df.head()\n",
        "\n",
        "# Drop the 'Unnamed: 0' column from the original dataframe\n",
        "vistara_df = vistara_df.drop(columns=['Unnamed: 0'])\n",
        "\n",
        "# One-hot encoding categorical variables\n",
        "vistara_df_encoded = pd.get_dummies(vistara_df, drop_first=True)\n",
        "\n",
        "# Handle missing values (if any)\n",
        "vistara_df_encoded = vistara_df_encoded.dropna()\n",
        "\n",
        "# Define features (X) and target variable (y)\n",
        "X_vistara = vistara_df_encoded.drop('price', axis=1)  # All features except 'price'\n",
        "y_vistara = vistara_df_encoded['price']  # Target variable: price\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Split the data into training and testing sets (80% train, 20% test)\n",
        "X_train_vistara, X_test_vistara, y_train_vistara, y_test_vistara = train_test_split(X_vistara, y_vistara, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize the Random Forest Regressor model\n",
        "rf_vistara_model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
        "\n",
        "# Train the model\n",
        "rf_vistara_model.fit(X_train_vistara, y_train_vistara)\n",
        "\n",
        "# Make predictions\n",
        "y_pred_vistara = rf_vistara_model.predict(X_test_vistara)\n",
        "\n",
        "# Evaluate the model (Mean Squared Error)\n",
        "mse_vistara = mean_squared_error(y_test_vistara, y_pred_vistara)\n",
        "print(f\"Mean Squared Error for Vistara: {mse_vistara}\")\n",
        "\n",
        "# Feature importance for Vistara flight ticket price prediction\n",
        "importances_vistara = rf_vistara_model.feature_importances_\n",
        "\n",
        "# Create a DataFrame with feature names and their importance scores\n",
        "feature_importance_vistara_df = pd.DataFrame({\n",
        "    'Feature': X_vistara.columns,\n",
        "    'Importance': importances_vistara\n",
        "})\n",
        "\n",
        "# Sort the features by importance\n",
        "feature_importance_vistara_df = feature_importance_vistara_df.sort_values(by='Importance', ascending=False)\n",
        "\n",
        "# Print top features\n",
        "print(\"Top Features Influencing Vistara Flight Price:\")\n",
        "print(feature_importance_vistara_df.head())\n",
        "\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "\n",
        "# Define a smaller set of hyperparameters to search over\n",
        "param_grid = {\n",
        "    'n_estimators': [100, 200],  # Number of trees in the forest\n",
        "    'max_depth': [None, 10, 20],  # Depth of each tree (controls overfitting)\n",
        "    'min_samples_split': [2, 10],  # Minimum samples to split an internal node\n",
        "    'min_samples_leaf': [1, 2],  # Minimum samples at each leaf node\n",
        "    'bootstrap': [True, False]  # Whether to sample data with replacement\n",
        "}\n",
        "\n",
        "# Initialize Random Forest model\n",
        "rf_model = RandomForestRegressor(random_state=77)\n",
        "\n",
        "# Initialize RandomizedSearchCV to search over parameter grid\n",
        "random_search = RandomizedSearchCV(estimator=rf_model, param_distributions=param_grid,\n",
        "                                   n_iter=5, cv=3, verbose=2, random_state=77, n_jobs=-1)\n",
        "\n",
        "# Fit the model to the data\n",
        "random_search.fit(X_train_vistara, y_train_vistara)\n",
        "\n",
        "# Output the best parameters found by the search\n",
        "print(f\"Best Hyperparameters: {random_search.best_params_}\")\n",
        "\n",
        "# Get the best model from the search\n",
        "best_rf_model = random_search.best_estimator_\n",
        "\n",
        "# Evaluate the best model on the test data\n",
        "y_pred_best = best_rf_model.predict(X_test_vistara)\n",
        "\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Calculate Mean Squared Error to evaluate the model's accuracy\n",
        "mse_best = mean_squared_error(y_test_vistara, y_pred_best)\n",
        "\n",
        "# Print the MSE\n",
        "print(f\"Mean Squared Error after Hyperparameter Tuning: {mse_best}\")\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "# Create a DataFrame to compare actual vs predicted prices for the test data\n",
        "comparison_df = pd.DataFrame({\n",
        "    'Actual Price': y_test_vistara,  # Actual prices from the test data\n",
        "    'Predicted Price': y_pred_best   # Predicted prices from the model\n",
        "})\n",
        "\n",
        "# Display the comparison of actual and predicted prices\n",
        "print(comparison_df.head())  # Print the first few rows of the comparison\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Scatter plot of actual vs predicted prices\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.scatter(y_test_vistara, y_pred_best, color='blue', alpha=0.6)  # Actual vs Predicted\n",
        "plt.plot([min(y_test_vistara), max(y_test_vistara)], [min(y_test_vistara), max(y_test_vistara)], color='red', linestyle='--')  # Ideal line\n",
        "plt.title(\"Actual vs Predicted Prices for Vistara Flights\")\n",
        "plt.xlabel(\"Actual Prices\")\n",
        "plt.ylabel(\"Predicted Prices\")\n",
        "plt.show()\n",
        "\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
        "\n",
        "# Calculate MAE\n",
        "mae = mean_absolute_error(y_test_vistara, y_pred_best)\n",
        "print(f\"Mean Absolute Error (MAE): {mae}\")\n",
        "\n",
        "print(best_rf_model)\n",
        "\n",
        "from sklearn.tree import plot_tree\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Select a single tree from the Random Forest\n",
        "plt.figure(figsize=(20, 10))\n",
        "plot_tree(best_rf_model.estimators_[0], filled=True, feature_names=X_train_vistara.columns, rounded=True)\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "57QEgFW6QtFk",
        "outputId": "f2fed5a0-3032-43c7-f616-c9d846f8972f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Index(['Unnamed: 0', 'airline', 'flight', 'source_city', 'departure_time',\n",
            "       'stops', 'arrival_time', 'destination_city', 'class', 'duration',\n",
            "       'days_left', 'price'],\n",
            "      dtype='object')\n",
            "Most Expensive Airline:\n",
            "airline\n",
            "Vistara    30396.536302\n",
            "Name: price, dtype: float64\n",
            "Mean Squared Error for Vistara: 10472040.299447142\n",
            "Top Features Influencing Vistara Flight Price:\n",
            "                    Feature  Importance\n",
            "155           class_Economy    0.861760\n",
            "0                  duration    0.061344\n",
            "1                 days_left    0.020944\n",
            "151  destination_city_Delhi    0.005136\n",
            "135       source_city_Delhi    0.004289\n",
            "Fitting 3 folds for each of 5 candidates, totalling 15 fits\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Gradient Boost Regressor**"
      ],
      "metadata": {
        "id": "YsMUPf6uSa8q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the dataset again\n",
        "file_path = \"Clean_Dataset.csv\"\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "# Step 1: Apply filters\n",
        "df_filtered = df[\n",
        "    (df[\"stops\"] == \"zero\") &  # Direct flights only\n",
        "    (df[\"source_city\"].isin([\"Delhi\", \"Mumbai\"])) &  # Major metro cities\n",
        "    (df[\"class\"] == \"Economy\")  # Economy class only\n",
        "]\n",
        "\n",
        "# Step 2: Limit to 50,000 rows if more are available\n",
        "df_filtered = df_filtered.sample(n=min(50000, len(df_filtered)), random_state=42)\n",
        "\n",
        "# Step 3: Drop unnecessary columns\n",
        "df_filtered = df_filtered.drop(columns=[\"Unnamed: 0\", \"flight\"])\n",
        "\n",
        "# Step 4: Encode categorical variables using Label Encoding\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "\n",
        "categorical_columns = [\"airline\", \"source_city\", \"departure_time\", \"stops\", \"arrival_time\", \"destination_city\", \"class\"]\n",
        "label_encoders = {}\n",
        "for col in categorical_columns:\n",
        "    le = LabelEncoder()\n",
        "    df_filtered[col] = le.fit_transform(df_filtered[col])\n",
        "    label_encoders[col] = le\n",
        "\n",
        "# Step 5: Define features and target variable\n",
        "X = df_filtered.drop(columns=[\"price\"])\n",
        "y = df_filtered[\"price\"]\n",
        "\n",
        "# Step 6: Scale numerical features\n",
        "scaler = StandardScaler()\n",
        "X[[\"duration\", \"days_left\"]] = scaler.fit_transform(X[[\"duration\", \"days_left\"]])\n",
        "\n",
        "# Step 7: Train-test split (80% train, 20% test)\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=45)\n",
        "\n",
        "# Step 8: Train Gradient Boosting Regressor\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "\n",
        "gbr = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, random_state=45)\n",
        "gbr.fit(X_train, y_train)\n",
        "\n",
        "\n",
        "# Step 9: Make predictions\n",
        "y_pred_gbr = gbr.predict(X_test)\n",
        "\n",
        "# Step 10: Evaluate model performance\n",
        "def evaluate_model(y_true, y_pred, model_name):\n",
        "    mae = mean_absolute_error(y_true, y_pred)\n",
        "    mse = mean_squared_error(y_true, y_pred)\n",
        "    r2 = r2_score(y_true, y_pred)\n",
        "    return {\"Model\": model_name, \"MAE\": mae, \"MSE\": mse, \"R² Score\": r2}\n",
        "\n",
        "gbr_results = evaluate_model(y_test, y_pred_gbr, \"Gradient Boosting Regressor (Filtered Data)\")\n",
        "\n",
        "# Display results using pandas directly\n",
        "gbr_results_df"
      ],
      "metadata": {
        "id": "mlf986aKSXoM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Support Vector Machine**"
      ],
      "metadata": {
        "id": "XFQ-2jJpS2YY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.svm import LinearSVR\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "\n",
        "df = pd.read_csv(\"Clean_Dataset.csv\")\n",
        "\n",
        "df.columns\n",
        "\n",
        "df = df.drop(columns = ['flight', 'Unnamed: 0'], axis = 1 )\n",
        "\n",
        "# Define important trained variables (features) and target variable\n",
        "trained_variables = ['airline', 'source_city', 'departure_time', 'stops', 'destination_city', 'class', 'duration', 'days_left']\n",
        "target_variable = 'price'\n",
        "\n",
        "# Apply one-hot encoding to categorical features\n",
        "categorical_features = ['airline', 'source_city', 'departure_time', 'stops', 'destination_city', 'class','arrival_time']\n",
        "ohe = OneHotEncoder(sparse_output=False, drop='first', handle_unknown='ignore')\n",
        "encoded_features = ohe.fit_transform(df[categorical_features])\n",
        "encoded_df = pd.DataFrame(encoded_features, columns=ohe.get_feature_names_out(categorical_features))\n",
        "\n",
        "# Reset index and ensure proper merging\n",
        "df = df.drop(columns=categorical_features).reset_index(drop=True)\n",
        "df = pd.concat([df, encoded_df], axis=1)\n",
        "\n",
        "# Convert all columns to float64 to avoid type mismatch\n",
        "df = df.astype('float64')\n",
        "\n",
        "# Apply log transformation to the target variable\n",
        "df[target_variable] = np.log1p(df[target_variable])\n",
        "\n",
        "# Prepare dataset\n",
        "X = df.drop(columns=[target_variable]).values\n",
        "y = df[target_variable].values\n",
        "\n",
        "# Split dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Standardize the dataset\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Define an optimized Linear SVR model\n",
        "svm_model = LinearSVR(C=1.0, max_iter=1000, random_state=42)\n",
        "\n",
        "svm_model.fit(X_train, y_train)\n",
        "\n",
        "# Predictions\n",
        "y_pred = svm_model.predict(X_test)\n",
        "\n",
        "# Evaluation\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "print(f\"Mean Squared Error: {mse:.2f}\")\n",
        "print(f\"R-squared Score: {r2:.2f}\")\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "\n",
        "# Add predictions to the test set for visualization\n",
        "test_results = pd.DataFrame({'Actual': y_test, 'Predicted': y_pred})\n",
        "\n",
        "# Function to plot predicted vs actual values\n",
        "def plot_predicted_vs_actual(df, actual_column, predicted_column):\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.scatter(df[actual_column], df[predicted_column], alpha=0.6, color='blue', label='Predicted vs Actual')\n",
        "    plt.plot([df[actual_column].min(), df[actual_column].max()],\n",
        "             [df[actual_column].min(), df[actual_column].max()],\n",
        "             color='red', lw=2, label='Ideal Fit')\n",
        "    plt.title('Predicted vs Actual Values')\n",
        "    plt.xlabel('Actual Values')\n",
        "    plt.ylabel('Predicted Values')\n",
        "    plt.legend()\n",
        "    plt.grid(alpha=0.3)\n",
        "    plt.show()\n",
        "\n",
        "# Plot the predicted vs actual values\n",
        "plot_predicted_vs_actual(test_results, 'Actual', 'Predicted')\n",
        "\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "import numpy as np\n",
        "\n",
        "# Convert continuous predictions to binary class labels\n",
        "y_pred_binary = np.where(y_pred >= 9, 1, 0)\n",
        "y_test_binary = np.where(y_test >= 9, 1, 0)\n",
        "\n",
        "# Generate confusion matrix\n",
        "conf_matrix = confusion_matrix(y_test_binary, y_pred_binary)\n",
        "\n",
        "# Function to plot the confusion matrix\n",
        "def plot_confusion_matrix(conf_matrix, class_names):\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names)\n",
        "    plt.title('Confusion Matrix')\n",
        "    plt.xlabel('Predicted Labels')\n",
        "    plt.ylabel('True Labels')\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "# Plot confusion matrix (code from earlier)\n",
        "plot_confusion_matrix(conf_matrix, class_names=['Class 0', 'Class 1'])\n",
        "\n",
        "# Print classification report\n",
        "print(\"Classification Report:\\n\", classification_report(y_test_binary, y_pred_binary))\n"
      ],
      "metadata": {
        "id": "yMnn6LdTS-T6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Neural Network**"
      ],
      "metadata": {
        "id": "VoVoMVhUTyKv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#importing Required Libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler,OneHotEncoder\n",
        "from sklearn.impute import SimpleImputer\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.layers import Dropout,Input\n",
        "from tensorflow.keras.regularizers import l2\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Dense, BatchNormalization, LeakyReLU, Dropout\n",
        "\n",
        "df = pd.read_csv(\"Clean_Dataset.csv\")\n",
        "\n",
        "df['price'].mean()\n",
        "df = df.drop(columns = ['flight', 'Unnamed: 0'], axis = 1 )\n",
        "\n",
        "# Define important trained variables (features) and target variable\n",
        "trained_variables = ['airline', 'source_city', 'departure_time', 'stops', 'destination_city', 'class', 'duration', 'days_left']\n",
        "target_variable = 'price'\n",
        "\n",
        "# Apply one-hot encoding to categorical features\n",
        "categorical_features = ['airline', 'source_city', 'departure_time', 'stops', 'destination_city', 'class','arrival_time']\n",
        "ohe = OneHotEncoder(sparse_output=False, drop='first', handle_unknown='ignore')\n",
        "encoded_features = ohe.fit_transform(df[categorical_features])\n",
        "encoded_df = pd.DataFrame(encoded_features, columns=ohe.get_feature_names_out(categorical_features))\n",
        "\n",
        "# Combine encoded categorical features with numerical features\n",
        "df = df.drop(columns=categorical_features)\n",
        "df = pd.concat([df, encoded_df], axis=1)\n",
        "\n",
        "# Apply log transformation to the target variable\n",
        "df[target_variable] = np.log1p(df[target_variable])\n",
        "\n",
        "# Prepare dataset\n",
        "X = df.drop(columns=[target_variable]).values\n",
        "y = df[target_variable].values\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Standardize the dataset\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Define an improved neural network model\n",
        "model = Sequential([\n",
        "    Dense(256, input_shape=(X_train.shape[1],)),\n",
        "    BatchNormalization(),\n",
        "    LeakyReLU(),\n",
        "    Dropout(0.2),\n",
        "    Dense(128),\n",
        "    BatchNormalization(),\n",
        "    LeakyReLU(),\n",
        "    Dropout(0.2),\n",
        "    Dense(64),\n",
        "    BatchNormalization(),\n",
        "    LeakyReLU(),\n",
        "    Dropout(0.2),\n",
        "    Dense(1, activation='linear')  # Regression output\n",
        "])\n",
        "\n",
        "# Define an improved neural network model\n",
        "model = Sequential([\n",
        "    Dense(256, kernel_regularizer=l2(0.001), input_shape=(X_train.shape[1],)),\n",
        "    BatchNormalization(),\n",
        "    LeakyReLU(),\n",
        "    Dropout(0.2),\n",
        "    Dense(128, kernel_regularizer=l2(0.001)),\n",
        "    BatchNormalization(),\n",
        "    LeakyReLU(),\n",
        "    Dropout(0.2),\n",
        "    Dense(64, kernel_regularizer=l2(0.001)),\n",
        "    BatchNormalization(),\n",
        "    LeakyReLU(),\n",
        "    Dropout(0.2),\n",
        "    Dense(1, activation='linear')  # Regression output\n",
        "])\n",
        "\n",
        "# Compile model\n",
        "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0005), loss='mse', metrics=['mae'])\n",
        "\n",
        "# Model summary\n",
        "model.summary()\n",
        "\n",
        "# Train the model\n",
        "# Train the model with Early Stopping\n",
        "callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
        "history = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=100, batch_size=64, callbacks=[callback])\n",
        "\n",
        "# Evaluate the model\n",
        "test_loss, test_mae = model.evaluate(X_test, y_test, verbose=1)\n",
        "print(f\"Test MAE: {test_mae}\")\n",
        "\n",
        "# Plot training and validation loss\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(history.history['loss'], label='Training Loss')\n",
        "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Training vs Validation Loss')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "\n",
        "# Plot training and validation MAE\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(history.history['mae'], label='Training MAE')\n",
        "plt.plot(history.history['val_mae'], label='Validation MAE')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('MAE')\n",
        "plt.title('Training vs Validation MAE')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "Xy7QnLKAT2Fn"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}